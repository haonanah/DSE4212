{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install swig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install quantstats\n",
    "# !pip install scikit-learn\n",
    "# !pip install torch\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vmdpy\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import GroupByScaler\n",
    "from utils import PortfolioOptimizationEnv, custom_reward_function, sharpe_ratio_reward_function\n",
    "from finrl.agents.portfolio_optimization.models import DRLAgent\n",
    "from finrl.agents.portfolio_optimization.architectures import EIIE\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (27190, 8)\n"
     ]
    }
   ],
   "source": [
    "TEST_SET = [\n",
    "    \"AAPL\", \"CVX\", \"GS\", \"JNJ\",\n",
    "    \"JPM\", \"MSFT\", \"PFE\", \"PG\",\n",
    "    \"GOOG\", \"XOM\"\n",
    "]\n",
    "START_DATE = '2014-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "print(len(TEST_SET))\n",
    "\n",
    "##COVARIATE 1: PRICES\n",
    "\n",
    "portfolio_raw_df = YahooDownloader(start_date = START_DATE,\n",
    "                                end_date = END_DATE,\n",
    "                                ticker_list = TEST_SET).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##COVARIATES 2,3: VIX, FEAR/GREED INDEX\n",
    "\n",
    "combined_spy_vix_fear_greed = pd.read_csv('datasets/VIX_feargreed/spy_vix_fear_greed_2011_2023.csv')\n",
    "\n",
    "\n",
    "portfolio_raw_df['date'] = pd.to_datetime(portfolio_raw_df['date'])\n",
    "combined_spy_vix_fear_greed['Date'] = pd.to_datetime(combined_spy_vix_fear_greed['Date'])\n",
    "\n",
    "\n",
    "df_portfolio_comb = pd.merge(portfolio_raw_df, combined_spy_vix_fear_greed, left_on='date', right_on='Date', how='left')\n",
    "\n",
    "\n",
    "df_portfolio_comb = df_portfolio_comb.drop(columns=['Date'])\n",
    "df_portfolio_comb['date'] = df_portfolio_comb['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##COVARIATE 4: STOCK HISTORICAL RATINGS\n",
    "\n",
    "FMP_historical_ratings = pd.read_csv('datasets/historical_ratings/FMP_historical_ratings.csv')\n",
    "\n",
    "df_portfolio_comb_2 = df_portfolio_comb.merge(\n",
    "    FMP_historical_ratings[['date', 'symbol', 'ratingScore', 'ratingDetailsDCFScore', 'ratingDetailsROEScore', 'ratingDetailsROAScore', 'ratingDetailsPEScore', 'ratingDetailsPBScore']],\n",
    "    left_on=['date', 'tic'],\n",
    "    right_on=['date', 'symbol'],\n",
    "    how='left' )\n",
    "\n",
    "df_portfolio_comb_2.drop(columns=['symbol'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##COVARIATE 5: Macro Indicators\n",
    "\n",
    "expenses = pd.read_csv('datasets/macro_indicators/expenses.csv')\n",
    "gdp = pd.read_csv('datasets/macro_indicators/gdp.csv')\n",
    "goods_services = pd.read_csv('datasets/macro_indicators/goods_and_services.csv')\n",
    "pound_dollar = pd.read_csv('datasets/macro_indicators/pound-dollar-exchange-rate-historical-chart.csv')\n",
    "unemployment = pd.read_csv('datasets/macro_indicators/unemployment.csv')\n",
    "index_consumer_services = pd.read_csv('datasets/macro_indicators/index_consumer_services.csv')\n",
    "\n",
    "df_portfolio_comb_2['date'] = pd.to_datetime(df_portfolio_comb_2['date'])\n",
    "expenses['date'] = pd.to_datetime(unemployment['date'])\n",
    "gdp['date'] = pd.to_datetime(gdp['DATE'])\n",
    "goods_services['date'] = pd.to_datetime(goods_services['date'])\n",
    "pound_dollar['date'] = pd.to_datetime(pound_dollar['date'])\n",
    "unemployment['date'] = pd.to_datetime(unemployment['date'])\n",
    "index_consumer_services['date'] = pd.to_datetime(index_consumer_services['date'])\n",
    "\n",
    "df_portfolio_comb_3 = pd.merge(df_portfolio_comb_2, expenses, on='date', how='left')\n",
    "df_portfolio_comb_3 = pd.merge(df_portfolio_comb_3, gdp, on='date', how='left')\n",
    "df_portfolio_comb_3 = pd.merge(df_portfolio_comb_3, goods_services, on='date', how='left')\n",
    "df_portfolio_comb_3 = pd.merge(df_portfolio_comb_3, pound_dollar, on='date', how='left')\n",
    "df_portfolio_comb_3 = pd.merge(df_portfolio_comb_3, unemployment, on='date', how='left')\n",
    "df_portfolio_comb_3 = pd.merge(df_portfolio_comb_3, index_consumer_services, on='date', how='left')\n",
    "\n",
    "# Sort the dataframe by date (and optionally 'tic' if needed)\n",
    "df_portfolio_comb_3 = df_portfolio_comb_3.sort_values(by=['date', 'tic'])\n",
    "\n",
    "# Forward fill missing data\n",
    "df_portfolio_comb_3['expenses'] = df_portfolio_comb_3['expenses'].ffill()\n",
    "df_portfolio_comb_3['GDP'] = df_portfolio_comb_3['GDP'].ffill()\n",
    "df_portfolio_comb_3['exports'] = df_portfolio_comb_3['exports'].ffill()\n",
    "df_portfolio_comb_3['imports'] = df_portfolio_comb_3['imports'].ffill()\n",
    "df_portfolio_comb_3['pound_dollar_exchange_rate'] = df_portfolio_comb_3['pound_dollar_exchange_rate'].ffill()\n",
    "df_portfolio_comb_3['unemployment'] = df_portfolio_comb_3['unemployment'].ffill()\n",
    "df_portfolio_comb_3['ics'] = df_portfolio_comb_3['ics'].ffill()\n",
    "df_portfolio_comb_3['net_export_goods_and_services'] = df_portfolio_comb_3['net_export_goods_and_services'].ffill()\n",
    "df_portfolio_comb_3['exports_goods'] = df_portfolio_comb_3['exports_goods'].ffill()\n",
    "df_portfolio_comb_3['exports_services'] = df_portfolio_comb_3['exports_services'].ffill()\n",
    "df_portfolio_comb_3['imports_goods'] = df_portfolio_comb_3['imports_goods'].ffill()\n",
    "df_portfolio_comb_3['imports_services'] = df_portfolio_comb_3['imports_services'].ffill()\n",
    "\n",
    "df_portfolio_comb_3.drop(columns=['DATE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_20624\\2437406948.py:11: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  DBITDA_ratio['date'] = pd.to_datetime(DBITDA_ratio['date'])\n",
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_20624\\2437406948.py:12: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  PB_ratio['date'] = pd.to_datetime(PB_ratio['date'])\n",
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_20624\\2437406948.py:13: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  PE_ratio['date'] = pd.to_datetime(PE_ratio['date'])\n",
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_20624\\2437406948.py:14: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  PS_ratio['date'] = pd.to_datetime(PS_ratio['date'])\n",
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_20624\\2437406948.py:15: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  quarterly_data['date'] = pd.to_datetime(quarterly_data['date'])\n",
      "C:\\Users\\limyi\\AppData\\Local\\Temp\\ipykernel_20624\\2437406948.py:16: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  ROE['date'] = pd.to_datetime(ROE['date'])\n"
     ]
    }
   ],
   "source": [
    "##COVARIATE 6: Fundamental Indicators\n",
    "\n",
    "DBITDA_ratio = pd.read_csv('datasets/fundemental indicators/DBITDA_ratio.csv')\n",
    "PB_ratio = pd.read_csv('datasets/fundemental indicators/PB_ratio.csv')\n",
    "PE_ratio = pd.read_csv('datasets/fundemental indicators/PE_ratio.csv')\n",
    "PS_ratio = pd.read_csv('datasets/fundemental indicators/PS_ratio.csv')\n",
    "quarterly_data = pd.read_csv('datasets/fundemental indicators/quaterlydata.csv')\n",
    "ROE = pd.read_csv('datasets/fundemental indicators/ROE.csv')\n",
    "\n",
    "df_portfolio_comb_3['date'] = pd.to_datetime(df_portfolio_comb_3['date'])\n",
    "DBITDA_ratio['date'] = pd.to_datetime(DBITDA_ratio['date'])\n",
    "PB_ratio['date'] = pd.to_datetime(PB_ratio['date'])\n",
    "PE_ratio['date'] = pd.to_datetime(PE_ratio['date'])\n",
    "PS_ratio['date'] = pd.to_datetime(PS_ratio['date'])\n",
    "quarterly_data['date'] = pd.to_datetime(quarterly_data['date'])\n",
    "ROE['date'] = pd.to_datetime(ROE['date'])\n",
    "\n",
    "df_portfolio_comb_4 = df_portfolio_comb_3\n",
    "\n",
    "df_portfolio_comb_4 = pd.merge(df_portfolio_comb_4, DBITDA_ratio, left_on=['tic', 'date'], right_on=['ticker', 'date'], how='left')\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.drop(columns=['ticker'])\n",
    "df_portfolio_comb_4 = pd.merge(df_portfolio_comb_4, PB_ratio, left_on=['tic', 'date'], right_on=['ticker', 'date'], how='left')\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.drop(columns=['ticker','Stock Price'])\n",
    "df_portfolio_comb_4 = pd.merge(df_portfolio_comb_4, PE_ratio, left_on=['tic', 'date'], right_on=['ticker', 'date'], how='left')\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.drop(columns=['ticker','Stock Price'])\n",
    "df_portfolio_comb_4 = pd.merge(df_portfolio_comb_4, PS_ratio, left_on=['tic', 'date'], right_on=['ticker', 'date'], how='left')\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.drop(columns=['ticker','Stock Price'])\n",
    "df_portfolio_comb_4 = pd.merge(df_portfolio_comb_4, quarterly_data, left_on=['tic', 'date'], right_on=['ticker', 'date'], how='left')\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.drop(columns=['ticker'])\n",
    "df_portfolio_comb_4 = pd.merge(df_portfolio_comb_4, ROE, left_on=['tic', 'date'], right_on=['ticker', 'date'], how='left')\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.drop(columns=['ticker'])\n",
    "\n",
    "# Sort the dataframe by date (and optionally 'tic' if needed)\n",
    "df_portfolio_comb_4 = df_portfolio_comb_4.sort_values(by=['tic', 'date'])\n",
    "\n",
    "# Forward fill missing data\n",
    "df_portfolio_comb_4['TTM Revenue(Billion)'] = df_portfolio_comb_4['TTM Revenue(Billion)'].ffill()\n",
    "df_portfolio_comb_4['TTM EBITDA(Billion)'] = df_portfolio_comb_4['TTM EBITDA(Billion)'].ffill()\n",
    "df_portfolio_comb_4['EBITDA Margin'] = df_portfolio_comb_4['EBITDA Margin'].ffill()\n",
    "df_portfolio_comb_4['Price to Book Ratio'] = df_portfolio_comb_4['Price to Book Ratio'].ffill()\n",
    "df_portfolio_comb_4['PE Ratio'] = df_portfolio_comb_4['PE Ratio'].ffill()\n",
    "df_portfolio_comb_4['Price to Sales Ratio'] = df_portfolio_comb_4['Price to Sales Ratio'].ffill()\n",
    "df_portfolio_comb_4['Assets'] = df_portfolio_comb_4['Assets'].ffill()\n",
    "df_portfolio_comb_4['NetIncomeLoss'] = df_portfolio_comb_4['NetIncomeLoss'].ffill()\n",
    "df_portfolio_comb_4['Return on Equity(%)'] = df_portfolio_comb_4['Return on Equity(%)'].ffill()\n",
    "df_portfolio_comb_4['Book Value per Share'] = df_portfolio_comb_4['Book Value per Share'].ffill()\n",
    "df_portfolio_comb_4['Net EPS'] = df_portfolio_comb_4['Net EPS'].ffill()\n",
    "df_portfolio_comb_4['TTM Sales per Share'] = df_portfolio_comb_4['TTM Sales per Share'].ffill()\n",
    "df_portfolio_comb_4['TTM Net Income (Billions)'] = df_portfolio_comb_4['TTM Net Income (Billions)'].ffill()\n",
    "df_portfolio_comb_4['Shareholder\\'s Equity (Billion)'] = df_portfolio_comb_4['Shareholder\\'s Equity (Billion)'].ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vmdpy import VMD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def apply_vmd_to_ticker_rolling(df, alpha=5000, tau=0, K=3, DC=0, init=1, tol=1e-7, window=365):\n",
    "    \"\"\"\n",
    "    Applies VMD to the 'close' price for each ticker in the dataframe using a rolling window approach.\n",
    "    This ensures that each VMD decomposition is based only on the previous `window` ticks, avoiding forward leakage.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe containing 'close' price data and 'tic' column.\n",
    "        alpha, tau, K, DC, init, tol: Parameters for the VMD decomposition.\n",
    "        window (int): Number of previous ticks to use for the rolling VMD.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the VMD modes added for each ticker.\n",
    "    \"\"\"\n",
    "    vmd_features = []\n",
    "    \n",
    "    # Apply VMD for each ticker\n",
    "    for tic in df['tic'].unique():\n",
    "        tic_df = df[df['tic'] == tic].copy()  # Filter data for the current ticker\n",
    "        close_prices = tic_df['close'].to_numpy()  # Extract close prices\n",
    "        \n",
    "        # Initialize empty columns for VMD modes\n",
    "        for k in range(K):\n",
    "            tic_df[f'vmd_mode_{k}'] = np.nan  # Initialize with NaNs\n",
    "\n",
    "        # Apply VMD in a rolling window manner\n",
    "        for i in range(window, len(close_prices)):\n",
    "            close_window = close_prices[i - window:i]  # Select the previous `window` close prices\n",
    "            \n",
    "            # Perform VMD on the window of close prices\n",
    "            u, _, _ = VMD(close_window, alpha, tau, K, DC, init, tol)\n",
    "            \n",
    "            # Assign the latest VMD modes (from the last tick of the window) to the current row\n",
    "            for k in range(K):\n",
    "                tic_df.at[tic_df.index[i], f'vmd_mode_{k}'] = u[k, -1]  # Assign last mode from the VMD result\n",
    "        \n",
    "        # Append the dataframe with VMD modes\n",
    "        vmd_features.append(tic_df)\n",
    "    \n",
    "    # Concatenate all VMD modes into one dataframe\n",
    "    vmd_df = pd.concat(vmd_features, axis=0)\n",
    "    \n",
    "    return vmd_df\n",
    "\n",
    "df_portfolio_comb_5 = apply_vmd_to_ticker_rolling(df_portfolio_comb_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portfolio_final = df_portfolio_comb_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of all columns in the DataFrame\n",
    "print(df_portfolio_final.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Str to Float64\n",
    "df_portfolio_final['net_export_goods_and_services'] = df_portfolio_final['net_export_goods_and_services'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['expenses'] = df_portfolio_final['expenses'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['exports'] = df_portfolio_final['exports'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['exports_goods'] = df_portfolio_final['exports_goods'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['exports_services'] = df_portfolio_final['exports_services'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['imports'] = df_portfolio_final['imports'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['imports_goods'] = df_portfolio_final['imports_goods'].str.replace(',', '').astype('float64')\n",
    "df_portfolio_final['imports_services'] = df_portfolio_final['imports_services'].str.replace(',', '').astype('float64')\n",
    "\n",
    "#Convert rest to Float64\n",
    "df_portfolio_final[df_portfolio_final.columns.difference(['date', 'tic'])] = df_portfolio_final[df_portfolio_final.columns.difference(['date', 'tic'])].astype('float64')\n",
    "\n",
    "#Convert date back\n",
    "df_portfolio_final['date'] = df_portfolio_final['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling NaN values with 0\n",
    "df_portfolio_final.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "portfolio_norm_df = GroupByScaler(by=\"tic\", scaler=MaxAbsScaler).fit_transform(df_portfolio_final)\n",
    "portfolio_norm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portfolio = portfolio_norm_df[[\"date\", \n",
    "                                  \"tic\", \n",
    "                                  \"close\",\n",
    "                                  \"vmd_mode_0\",\n",
    "                                 \"vmd_mode_1\",\n",
    "                                 \"vmd_mode_2\",\n",
    "                                  \"high\", \n",
    "                                  \"low\",\n",
    "                                  'High_VIX',\n",
    "                                  'Low_VIX',\n",
    "                                  'Close_VIX',\n",
    "                                  'High_SPY',\n",
    "                                  'Low_SPY',\n",
    "                                  'Close_SPY',\n",
    "                                  'Volume_SPY',\n",
    "                                  'Fear Greed', \n",
    "                                  'ratingScore',\n",
    "                                  'ratingDetailsDCFScore',\n",
    "                                  'ratingDetailsROEScore',\n",
    "                                  'ratingDetailsROAScore',\n",
    "                                  'ratingDetailsPEScore',\n",
    "                                  'ratingDetailsPBScore',\n",
    "                                  'expenses',\n",
    "                                  'GDP', \n",
    "                                  'exports', \n",
    "                                  'imports',\n",
    "                                  'pound_dollar_exchange_rate', \n",
    "                                  'unemployment',\n",
    "                                  'ics',\n",
    "                                  'TTM Revenue(Billion)',\n",
    "                                  'TTM EBITDA(Billion)',\n",
    "                                  'EBITDA Margin',\n",
    "                                  'Price to Book Ratio',\n",
    "                                  'PE Ratio',\n",
    "                                  'Price to Sales Ratio',\n",
    "                                  'Assets',\n",
    "                                  'NetIncomeLoss',\n",
    "                                  'Return on Equity(%)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_portfolio_train = df_portfolio[(df_portfolio[\"date\"] >= START_DATE) & (df_portfolio[\"date\"] < \"2022-12-31\")]\n",
    "df_portfolio_test = df_portfolio[(df_portfolio[\"date\"] >= \"2023-01-01\") & (df_portfolio[\"date\"] < END_DATE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save as parquet\n",
    "# df_portfolio_train.to_parquet('datasets/df_portfolio_train.parquet')\n",
    "# df_portfolio_2021.to_parquet('datasets/df_portfolio_2021.parquet')\n",
    "# df_portfolio_2022.to_parquet('datasets/df_portfolio_2022.parquet')\n",
    "# df_portfolio_2023.to_parquet('datasets/df_portfolio_2023.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and Initialise Packages\n",
    "If loading data start here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "# import torch\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# from sklearn.preprocessing import MaxAbsScaler\n",
    "# from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "# from finrl.meta.preprocessor.preprocessors import GroupByScaler\n",
    "# from utils import PortfolioOptimizationEnv, custom_reward_function, sharpe_ratio_reward_function\n",
    "# from finrl.agents.portfolio_optimization.models import DRLAgent\n",
    "# from finrl.agents.portfolio_optimization.architectures import EIIE\n",
    "\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# # Load the data\n",
    "# df_portfolio_train = pd.read_parquet('datasets/df_portfolio_train.parquet')\n",
    "# df_portfolio_2021 = pd.read_parquet('datasets/df_portfolio_2021.parquet')\n",
    "# df_portfolio_2022 = pd.read_parquet('datasets/df_portfolio_2022.parquet')\n",
    "# df_portfolio_2023 = pd.read_parquet('datasets/df_portfolio_2023.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_portfolio_2021.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and DRL Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feature_selection.ipynb, we perform a regression of parameters on close prices with LassoCV, and then inspect the coefficients to make a preliminary selection of the most efficient subset of features to include in our DRL model.\n",
    "\n",
    "The selected features are: \n",
    "\n",
    "- High_VIX, Low_VIX, High_SPY, Low_SPY, Close_SPY, \n",
    "- Fear Greed, ratingScore, ratingDetailsROEScore, \n",
    "- exports, pound_dollar_exchange_rate, unemployment, ics,\n",
    "- TTM Revenue(Billion), Price to Book Ratio, Price to Sales Ratio, Return on Equity(%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_NAMES = [\"close\",\n",
    "               \"vmd_mode_0\",\n",
    "                \"vmd_mode_1\",\n",
    "                \"vmd_mode_2\",\n",
    "                  #\"high\", \n",
    "                  #\"low\",\n",
    "                'High_VIX',\n",
    "                'Low_VIX',\n",
    "                  #'Close_VIX',\n",
    "                'High_SPY',\n",
    "                'Low_SPY',\n",
    "                'Close_SPY',\n",
    "                  #'Volume_SPY',\n",
    "                'Fear Greed', \n",
    "                'ratingScore',\n",
    "                  #'ratingDetailsDCFScore',\n",
    "                'ratingDetailsROEScore',\n",
    "                  #'ratingDetailsROAScore',\n",
    "                  #'ratingDetailsPEScore',\n",
    "                  #'ratingDetailsPBScore',\n",
    "                  #'expenses',\n",
    "                  #'GDP', \n",
    "                'exports', \n",
    "                  #'imports',\n",
    "                'pound_dollar_exchange_rate', \n",
    "                'unemployment',\n",
    "                'ics',\n",
    "                'TTM Revenue(Billion)',\n",
    "                  #'TTM EBITDA(Billion)',\n",
    "                  #'EBITDA Margin',\n",
    "                'Price to Book Ratio',\n",
    "                  #'PE Ratio',\n",
    "                'Price to Sales Ratio',\n",
    "                  #'Assets',\n",
    "                  #'NetIncomeLoss',\n",
    "                'Return on Equity(%)']\n",
    "\n",
    "TIME_WINDOW = 50\n",
    "COMISSION_FEE = 0.0025\n",
    "K_SIZE = 4\n",
    "CONV_MID = 5\n",
    "CONV_FINAL= 20\n",
    "\n",
    "environment = PortfolioOptimizationEnv(\n",
    "        df_portfolio_train,\n",
    "        initial_amount=100000,\n",
    "        comission_fee_pct=COMISSION_FEE,\n",
    "        time_window=TIME_WINDOW,\n",
    "        features=FEATURE_NAMES,\n",
    "        normalize_df=None,\n",
    "        reward_function=custom_reward_function,\n",
    "        reward_scaling=1.0\n",
    "    )\n",
    "\n",
    "# set PolicyGradient parameters\n",
    "model_kwargs = {\n",
    "    \"lr\": 0.01,\n",
    "    \"policy\": EIIE,\n",
    "}\n",
    "\n",
    "# here, we can set EIIE's parameters\n",
    "policy_kwargs = {\n",
    "    \"initial_features\": len(FEATURE_NAMES),\n",
    "    \"k_size\": K_SIZE,\n",
    "    \"time_window\": TIME_WINDOW,\n",
    "    \"conv_mid_features\":CONV_MID,\n",
    "    \"conv_final_features\":CONV_FINAL}\n",
    "\n",
    "EIIE_model = DRLAgent(environment).get_model(\"pg\", device, model_kwargs, policy_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Model Training (Jan 2014- Dec 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DRLAgent.train_model(EIIE_model, episodes=10)\n",
    "torch.save(EIIE_model.train_policy.state_dict(), \"policy_EIIE.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Model Evaluation (Jan 2023-Oct 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "environment_test = PortfolioOptimizationEnv(\n",
    "    df_portfolio_test,\n",
    "    initial_amount=100000,\n",
    "    comission_fee_pct=COMISSION_FEE,\n",
    "    time_window=TIME_WINDOW,\n",
    "    features=FEATURE_NAMES,\n",
    "    normalize_df=None,\n",
    "    reward_function=custom_reward_function,\n",
    "    reward_scaling=1.0\n",
    ")\n",
    "\n",
    "\n",
    "EIIE_results = {\n",
    "    \"train\": {},\n",
    "    \"test\": {} }\n",
    "\n",
    "# EI3_results = {\n",
    "#     \"train\": {},\n",
    "#     \"2021\": {},\n",
    "#     \"2022\": {},\n",
    "#     \"2023\": {}\n",
    "# }\n",
    "\n",
    "# instantiate an architecture with the same arguments used in training\n",
    "# and load with load_state_dict.\n",
    "\n",
    "EIIE_policy = EIIE(time_window = TIME_WINDOW, device = device, initial_features = len(FEATURE_NAMES),\n",
    "              k_size = K_SIZE, conv_mid_features = CONV_MID,conv_final_features = CONV_FINAL)\n",
    "\n",
    "EIIE_policy.load_state_dict(torch.load(\"policy_EIIE.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the final accumulative portfolio value\n",
    "def calculate_cumulative_value(portfolio_values):\n",
    "    initial_value = portfolio_values[0]\n",
    "    final_value = portfolio_values[-1]\n",
    "    return final_value / initial_value\n",
    "\n",
    "# Function to calculate the maximum drawdown\n",
    "def calculate_max_drawdown(portfolio_values):\n",
    "    portfolio_values = np.array(portfolio_values)\n",
    "    running_max = np.maximum.accumulate(portfolio_values)\n",
    "    drawdowns = (portfolio_values - running_max) / running_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "    \n",
    "    return max_drawdown\n",
    "\n",
    "# Function to calculate the Sharpe ratio\n",
    "def calculate_sharpe_ratio(portfolio_values, risk_free_rate=0):\n",
    "    returns = np.diff(portfolio_values) / portfolio_values[:-1]\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns)\n",
    "    sharpe_ratio_annualized = sharpe_ratio * np.sqrt(252)\n",
    "    \n",
    "    return sharpe_ratio_annualized\n",
    "\n",
    "\n",
    "# Define a function to calculate the metrics (this is just a placeholder; replace with actual functions)\n",
    "def calculate_metrics(environment):\n",
    "    final_portfolio_value = environment._asset_memory[\"final\"][-1]\n",
    "    max_drawdown = calculate_max_drawdown(environment._asset_memory[\"final\"])\n",
    "    sharpe_ratio = calculate_sharpe_ratio(environment._asset_memory[\"final\"])\n",
    "    cumulative_value = calculate_cumulative_value(environment._asset_memory[\"final\"])\n",
    "    return final_portfolio_value, max_drawdown, sharpe_ratio, cumulative_value\n",
    "\n",
    "# DataFrame to store results\n",
    "columns = [\"TIME_WINDOW\", \"K_SIZE\", \"CONV_MID\", \"CONV_FINAL\", \n",
    "        'train_portfolio_value', 'train_drawdown', 'train_sharpe', 'train_cumulative',\n",
    "          'test_portfolio_value', 'test_drawdown', 'test_sharpe', 'test_cumulative']\n",
    "\n",
    "results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "environment.reset()\n",
    "src_folder = \"results/rl\"\n",
    "dst_folder = \"results/train\"\n",
    "if not os.path.exists(src_folder):\n",
    "    os.makedirs(src_folder)\n",
    "if os.path.exists(dst_folder):\n",
    "    shutil.rmtree(dst_folder)\n",
    "DRLAgent.DRL_validation(EIIE_model, environment, policy=EIIE_policy)\n",
    "os.rename(src_folder, dst_folder)\n",
    "EIIE_results[\"train\"][\"value\"] = environment._asset_memory[\"final\"]\n",
    "EIIE_results[\"train\"][\"actions\"] = environment._actions_memory\n",
    "train_final_value, train_drawdown, train_sharpe, train_cumulative = calculate_metrics(environment)\n",
    "\n",
    "src_folder = \"results/rl\"\n",
    "dst_folder = \"results/test\"\n",
    "if not os.path.exists(src_folder):\n",
    "    os.makedirs(src_folder)\n",
    "if os.path.exists(dst_folder):\n",
    "    shutil.rmtree(dst_folder)\n",
    "DRLAgent.DRL_validation(EIIE_model, environment_test, policy=EIIE_policy)\n",
    "os.rename(src_folder, dst_folder)\n",
    "EIIE_results[\"test\"][\"value\"] = environment_test._asset_memory[\"final\"]\n",
    "EIIE_results[\"test\"][\"actions\"] = environment_test._actions_memory\n",
    "portfolio_test_value, drawdown_test, sharpe_test, cumulative_test = calculate_metrics(environment_test)\n",
    "\n",
    "\n",
    "# EI3_policy = EI3(time_window=50, device=device)\n",
    "# EI3_policy.load_state_dict(torch.load(\"policy_EI3.pt\"))\n",
    "\n",
    "# environment.reset()\n",
    "# DRLAgent.DRL_validation(EI3_model, environment, policy=EI3_policy)\n",
    "# EI3_results[\"train\"][\"value\"] = environment._asset_memory[\"final\"]\n",
    "# environment_2021.reset()\n",
    "# DRLAgent.DRL_validation(EI3_model, environment_2021, policy=EI3_policy)\n",
    "# EI3_results[\"2021\"][\"value\"] = environment_2021._asset_memory[\"final\"]\n",
    "# environment_2022.reset()\n",
    "# DRLAgent.DRL_validation(EI3_model, environment_2022, policy=EI3_policy)\n",
    "# EI3_results[\"2022\"][\"value\"] = environment_2022._asset_memory[\"final\"]\n",
    "# environment_2023.reset()\n",
    "# DRLAgent.DRL_validation(EI3_model, environment_2023, policy=EI3_policy)\n",
    "# EI3_results[\"2023\"][\"value\"] = environment_2023._asset_memory[\"final\"]\n",
    "\n",
    "#uniform buy n hold\n",
    "if not os.path.exists(src_folder):\n",
    "    os.makedirs(src_folder)\n",
    "UBAH_results = {\n",
    "    \"train\": {\"value\": [], \"actions\": []},\n",
    "    \"test\": {\"value\": [], \"actions\": []} }\n",
    "\n",
    "\n",
    "PORTFOLIO_SIZE = len(TEST_SET)\n",
    "\n",
    "# train period\n",
    "terminated = False\n",
    "environment.reset()\n",
    "while not terminated:\n",
    "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
    "    _, _, terminated, _ = environment.step(action)\n",
    "    UBAH_results[\"train\"][\"actions\"].append(action)\n",
    "UBAH_results[\"train\"][\"value\"] = environment._asset_memory[\"final\"]\n",
    "\n",
    "\n",
    "# test\n",
    "terminated = False\n",
    "environment_test.reset()\n",
    "while not terminated:\n",
    "    action = [0] + [1/PORTFOLIO_SIZE] * PORTFOLIO_SIZE\n",
    "    _, _, terminated, _ = environment_test.step(action)\n",
    "    UBAH_results[\"test\"][\"actions\"].append(action)\n",
    "UBAH_results[\"test\"][\"value\"] = environment_test._asset_memory[\"final\"]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "plt.plot(UBAH_results[\"train\"][\"value\"], label=\"Buy and Hold\")\n",
    "plt.plot(EIIE_results[\"train\"][\"value\"], label=\"EIIE\")\n",
    "#plt.plot(EI3_results[\"train\"][\"value\"], label=\"EI3\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(\"Performance in training period (2014-2020)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(UBAH_results[\"test\"][\"value\"], label=\"Buy and Hold\")\n",
    "plt.plot(EIIE_results[\"test\"][\"value\"], label=\"EIIE\")\n",
    "#plt.plot(EI3_results[\"2023\"][\"value\"], label=\"EI3\")\n",
    "\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(\"Performance in testing period (2023-2024)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Allocation by Trained DRL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to plot action memory\n",
    "def plot_action_memory(actions, tickers, label):\n",
    "    \"\"\"Visualize portfolio allocation over time.\"\"\"\n",
    "    actions_df = pd.DataFrame(actions, columns=[\"cash\"] + tickers)\n",
    "    actions_df.plot(kind=\"line\", figsize=(12, 8), title=f\"Portfolio Allocation Breakdown - {label}\")\n",
    "    plt.ylabel(\"Portfolio Weight\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot EIIE actions for the training period\n",
    "plot_action_memory(EIIE_results[\"train\"][\"actions\"], TEST_SET, \"EIIE - Train\")\n",
    "\n",
    "# Plot EIIE actions for test\n",
    "plot_action_memory(EIIE_results[\"test\"][\"actions\"], TEST_SET, \"EIIE - 2023-2024\")\n",
    "\n",
    "# Similarly, plot UBAH actions for each period\n",
    "# plot_action_memory(UBAH_results[\"train\"][\"actions\"], TEST_SET, \"UBAH - Train\")\n",
    "# plot_action_memory(UBAH_results[\"2021\"][\"actions\"], TEST_SET, \"UBAH - 2021\")\n",
    "# plot_action_memory(UBAH_results[\"2022\"][\"actions\"], TEST_SET, \"UBAH - 2022\")\n",
    "# plot_action_memory(UBAH_results[\"2023\"][\"actions\"], TEST_SET, \"UBAH - 2023\")\n",
    "\n",
    "temp_df = pd.DataFrame([{\n",
    "        \"TIME_WINDOW\": TIME_WINDOW, \"K_SIZE\": K_SIZE, \"CONV_MID\": CONV_MID, \"CONV_FINAL\": CONV_FINAL,\n",
    "        \"train_portfolio_value\": train_final_value, \"train_drawdown\": train_drawdown, \"train_sharpe\": train_sharpe, \"train_cumulative\": train_cumulative,\n",
    "        \"test_portfolio_value\": portfolio_test_value, \"test_drawdown\": drawdown_test, \"test_sharpe\": sharpe_test, \"test_cumulative\": cumulative_test}]\n",
    ")\n",
    "results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "    \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('filename.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
